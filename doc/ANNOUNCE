$Header: /data/zender/nco_20150216/nco/doc/ANNOUNCE,v 1.764 2015-01-30 04:16:46 zender Exp $ -*-text-*-

The netCDF Operators NCO version 4.4.8 are ready. 

http://nco.sf.net (Homepage)
http://dust.ess.uci.edu/nco (Homepage "mirror")

NCO now provides a major new feature: lossy compression.
This is distinct from the packing (scale_factor+add_offset) that
NCO has long supported. The new lossy compression feature is
activated by specifying desired level of precision in terms of
either the total number of significant digits or the number of
significant digits after (or before) the decimal point.
These precision features are lumped together under the generic
name Least Significant Digit (PPC). More details below.

Work on NCO 4.4.9 is underway, still focused on stability and speed.
This includes more netCDF4 mop-up and new chunking features.

Enjoy,
Charlie

NEW FEATURES (full details always in ChangeLog):

A. NCO warns when appended output type differs from input type. 
   Previously NCO would not warn or die when the user (usually
   inadvertently) wrote data of one type into a destination meant
   for a different type. These commands would therefore complete
   without warning:
   ncks -C -O -v double_var ~/nco/data/in.nc ~/foo.nc
   ncrename -O -v double_var,float_var ~/foo.nc
   ncks -C -A -v float_var ~/nco/data/in.nc ~/foo.nc
   Now the user is warned but the operations is still permitted.
   http://nco.sf.net/nco.html#-A

A. NCO will now store data at a specifiable level of precision.
   Standard terminology for precision is remarkably ambiguous, and 
   we call this Least Significant Digit (PPC) functionality.
   NCO's PPC implementation currently accepts two types of precision
   specification implemented with completely different algorithms.
   Users can specify either the total Number of Significant Digits
   (NSD) or the Decimal Significant Digits (DSD), mean the number of
   significant digits after (or before) the decimal point. 
   For example, NSD=5 tells NCO to retain 5 significant digits in the
   specified variables. Specifying DSD=3 or DSD=-2 causes NCO to
   preserve the number rounded to the nearest thousandth or hundred,
   respectively. In other words, DSD is the negative base 10 logarithm
   of the desired decimal precision. Under the hood, NSD uses
   bitmasking to eliminate unnecessary (false) precision, while
   DSD utilizes rounding. PPC (both NSD and DSD) intentionally
   loses information (aka unwanted precision).
   The rounding is
   performed by zeroing the greatest number of mantissa bits
   consistent with the desired precision. This results in consecutive 
   zeros occupying signicant portions least siginificant bit in
   the on-disk IEEE storage of floating point numbers. Byte-stream
   compression techniques, such as the gzip compression used
   by HDF5, will compress these strings of zeros more efficiently than 
   unrounded numbers. The net result is that netCDF4 files that
   utilize compression (e.g., ncks -L 1) can be significantly
   reduced in size. There is no benefit to rounding numbers and
   storing them in netCDF3 files because netCDF3 does not support
   compression and thus one may as well maintain the precision.
   This feature only works when the output dataset is netCDF4.
   ncks -4 --ppc temperature=3 --ppc pressure,speed=-2 in.nc out.nc
   http://nco.sf.net/nco.htlm#ppc

BUG FIXES:

A. Hi
   ncrename has known problems when renaming netCDF4 coordinates.
   These problems will go away when Unidata issues a fix to the
   underlying netCDF library. Thanks to Parker Allen for reporting.
   http://nco.sf.net#bug_nc4_rename

B. Fix ncks bug dumping files with multiple long dimension names.
   This means you, NASA AIRS :)

KNOWN PROBLEMS DUE TO NCO:

   This section of ANNOUNCE reports and reminds users of the
   existence and severity of known, not yet fixed, problems. 
   These problems occur with NCO 4.4.8 built/tested with netCDF
   4.3.3-rc2 (20141112) on top of HDF5 hdf5-1.8.13 with:

   cd ~/nco;./configure --enable-netcdf4  # Configure mechanism -or-
   cd ~/nco/bld;make dir;make allinone # Old Makefile mechanism

A. NOT YET FIXED (NCO problem)
   Correctly read arrays of NC_STRING with embedded delimiters in ncatted arguments

   Demonstration:
   ncatted -D 5 -O -a new_string_att,att_var,c,sng,"list","of","str,ings" ~/nco/data/in_4.nc ~/foo.nc
   ncks -m -C -v att_var ~/foo.nc

   20130724: Verified problem still exists
   TODO nco1102
   Cause: NCO parsing of ncatted arguments is not sophisticated
   enough to handle arrays of NC_STRINGS with embedded delimiters.

B. NOT YET FIXED (NCO problem?)
   ncra/ncrcat (not ncks) hyperslabbing can fail on variables with multiple record dimensions

   Demonstration:
   ncrcat -O -d time,0 ~/nco/data/mrd.nc ~/foo.nc

   20140826: Verified problem still exists
   20140619: Problem reported by rmla
   Cause: Unsure. Maybe ncra.c loop structure not amenable to MRD?
   Workaround: Convert to fixed dimensions then hyperslab

KNOWN PROBLEMS DUE TO BASE LIBRARIES/PROTOCOLS:

A. NOT YET FIXED (netCDF4 problem)
   Renaming netCDF4 coordinate variables or dimensions "succeeds" but  
   corrupts (sets to _FillValue) values in the output dataset.
   Full description here http://nco.sf.net#bug_nc4_rename

   Demonstration with netCDF <= 4.3.2:
   ncrename -O -v time,newrec ~/nco/data/in_grp.nc ~/foo.nc
   ncks --cdl -g // -v newrec -d time,0 -C ~/foo.nc

   20141007: Problem reported by Parker Norton
   20141008: Problem reported to Unidata
   20141010: Verified by Unidata.
   20141112: Verified problem still exists
   Bug tracking: https://www.unidata.ucar.edu/jira/browse/NCF-177
   Workaround: Convert to netCDF3, rename, convert back to netCDF4

B. NOT YET FIXED (netCDF4 or HDF5 problem?)
   Specifying strided hyperslab on large netCDF4 datasets leads
   to slowdown or failure with recent netCDF versions.

   Demonstration with NCO <= 4.4.5:
   time ncks -O -d time,0,,12 ~/ET_2000-01_2001-12.nc ~/foo.nc
   Demonstration with NCL:
   time ncl < ~/nco/data/ncl.ncl   
   20140718: Problem reported by Parker Norton
   20140826: Verified problem still exists
   20140930: Finish NCO workaround for problem
   Cause: Slow algorithm in nc_var_gets()?
   Workaround #1: Use NCO 4.4.6 or later (avoids nc_var_gets())
   Workaround #2: Convert file to netCDF3 first, then use stride

C. NOT YET FIXED (would require DAP protocol change?)
   Unable to retrieve contents of variables including period '.' in name
   Periods are legal characters in netCDF variable names.
   Metadata are returned successfully, data are not.
   DAP non-transparency: Works locally, fails through DAP server.

   Demonstration:
   ncks -O -C -D 3 -v var_nm.dot -p http://thredds-test.ucar.edu/thredds/dodsC/testdods in.nc # Fails to find variable

   20130724: Verified problem still exists. 
   Stopped testing because inclusion of var_nm.dot broke all test scripts.
   NB: Hard to fix since DAP interprets '.' as structure delimiter in HTTP query string.

   Bug tracking: https://www.unidata.ucar.edu/jira/browse/NCF-47

D. NOT YET FIXED (would require DAP protocol change)
   Correctly read scalar characters over DAP.
   DAP non-transparency: Works locally, fails through DAP server.
   Problem, IMHO, is with DAP definition/protocol

   Demonstration:
   ncks -O -D 1 -H -C -m --md5_dgs -v md5_a -p http://thredds-test.ucar.edu/thredds/dodsC/testdods in.nc

   20120801: Verified problem still exists
   Bug report not filed
   Cause: DAP translates scalar characters into 64-element (this
   dimension is user-configurable, but still...), NUL-terminated
   strings so MD5 agreement fails 

"Sticky" reminders:

A. Pre-built Debian Sid & Ubuntu packages:
   http://nco.sf.net#debian

B. Pre-built Fedora and CentOS RPMs:
   http://nco.sf.net#rpm

C. Pre-built Mac binaries:
   http://nco.sf.net#mac

D. Pre-built Windows (native) and Cygwin binaries:
   http://nco.sf.net#windows

E. Reminder that NCO works on most HDF4 and HDF5 datasets, e.g., 
   HDF4: AMSR MERRA MODIS ...
   HDF5: GLAS ICESat Mabel SBUV ...
   HDF-EOS5: AURA HIRDLS OMI ...

